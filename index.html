<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EMNLP 2025 Tutorial: Code Intelligence in Language Models</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
              <span style="font-size: 80%">EMNLP 2025 Tutorial</span><br />
              <i>NLP+Code:</i> Code Intelligence in Language Models</i>
            </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <table>
            <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="33%" style="text-align: center; padding: 3px"><img width="125px" height="125px" src="static/imgs/profile_terry.jpeg"></td>
                <td width="33%" style="text-align: center; padding: 3px"><img width="125px" height="125px" src="static/imgs/profile_qian.png"></td>
                <td width="33%" style="text-align: center; padding: 3px"><img width="125px" height="125px" src="static/imgs/profile_zijian.jpeg"></td>
            </tr>
              <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="33%" style="text-align: center"><a href="https://terryyz.github.io/" style="border-radius: 50%; white-space: nowrap;">Terry Yue Zhuo</a><sup>1,2</sup>,</td>
                <td width="33%" style="text-align: center"><a href="https://siviltaram.github.io/" style="border-radius: 50%; white-space: nowrap;">Qian Liu</a><sup>3</sup>,</td>
                <td width="33%" style="text-align: center"><a href="https://zijianwang.me/" style="border-radius: 50%; white-space: nowrap;">Zijian Wang</a><sup>4</sup></td>
              </tr>
              <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="33%" style="text-align: center; padding: 3px"><img width="125px" height="125px" src="static/imgs/profile_wasi.png"></td>
                <td width="33%" style="text-align: center; padding: 3px"><img width="150px" height="120px" src="static/imgs/profile_binyuan.jpg" style="object-fit: cover; object-position: center;"></td>
                <td width="33%" style="text-align: center; padding: 3px"><img width="125px" height="125px" src="static/imgs/profile_loubna.jpeg"></td>
            </tr>
              <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="33%" style="text-align: center"><a href="https://wasiahmad.github.io/" style="border-radius: 0%; white-space: nowrap;">Wasi Uddin Ahmad</a><sup>5</sup>,</td>
                <td width="33%" style="text-align: center"><a href="https://huybery.github.io/" style="border-radius: 50%; white-space: nowrap;">Binyuan Hui</a><sup>6</sup>,</td>
                <td width="33%" style="text-align: center"><a href="https://loubnabnl.github.io/" style="border-radius: 50%; white-space: nowrap;">Loubna Ben Allal</a><sup>7</sup></td>
              </tr>
            </table>
              <!-- <a href="https://akariasai.github.io/">Akari Asai</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://shmsw25.github.io/">Sewon Min</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.princeton.edu/~zzhong/">Zexuan Zhong</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a><sup>2</sup>, -->
            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Monash University,</span>
            <span class="author-block"><sup>2</sup>CSIRO's Data61,</span>
            <span class="author-block"><sup>3</sup>ByteDance</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>4</sup>Meta,</span>
            <span class="author-block"><sup>5</sup>NVIDIA,</span>
            <span class="author-block"><sup>6</sup>Alibaba Group,</span>
            <span class="author-block"><sup>7</sup>Hugging Face</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><code>code-lm@googlegroups.com</code></span>
          </div>

          <br />
          <div class="is-size-5 publication-authors">
            <b>Saturday, Nov 8, 9:00 - 12:30 @ Suzhou International Expo Centre</b>
          </div>
          
          <!--<div class="is-size-6 publication-authors">
            For those who have not registered to ACL: we will release video recordings after the tutorial
          </div>
          <br />-->
          <div class="is-size-5 publication-authors">
            QnA: <a href="https://tinyurl.com/code-lm-tutorial" target="_blank"><b>tinyurl.com/code-lm-tutorial</b></a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this tutorial</h2>
        <div class="content has-text-justified">
          <!--<p>
            Language models (LMs) such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) have shown impressive abilities in a range of natural language processing (NLP) tasks. 
            However, relying solely on their parameters to encode a wealth of world knowledge requires a prohibitively large number of parameters and hence massive computing, and they often struggle to learn long-rail knowledge (Roberts et al., 2020; Kandpal et al., 2022; Mallen et al., 2022). 
            Moreover, these parametric LMs are fundamentally incapable of adapting over time (De Cao et al., 2021; Lazaridou et al., 2021; Kasai et al., 2022), often hallucinate (Shuster et al., 2021), and may leak private data from the
            training corpus (Carlini et al., 2021). To overcome these limitations, there has been growing interest in retrieval-based LMs (Guu et al., 2020; Khandelwal et al., 2020; Borgeaud et al., 2022; Zhong et al., 2022; Izacard et al., 2022b; Min et al., 2022),
            which incorporate a non-parametric datastore (e.g., text chunks from an external corpus) with their parametric counterparts. Retrieval-based LMs can outperform LMs without retrieval by a large margin with much fewer parameters (Mallen et al., 2022), can update their knowledge by replacing their retrieval corpora (Izacard et al., 2022b), and provide citations for users to easily verify and evaluate the predictions (Menick et al., 2022; Bohnet et al., 2022).
          </p>
          <p>
            In this tutorial, we aim to provide a comprehensive and coherent overview of recent
            advances in retrieval-based LMs. We will start
            by first providing preliminaries covering the foundations of LM (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search methods widely used in neural retrieval systems; Karpukhin et al. 2020). We will then focus
            on recent progress in architectures, learning approaches, and applications of retrieval-based LMs.
          </p>-->
          <p>
            Language models (LMs) like GPT and Claude have shown impressive abilities in a range of natural language processing (NLP) tasks. Among these tasks, code understanding and generation have quickly become one of the most popular applications of LMs, given its nature of executable logic forms. However, there is a practical understanding of how programming knowledge can be combined with natural language to automate software development. Moreover, recent studies also empirically demonstrate that code can be a better form for complex reasoning and agentic task automation, but they do not indicate their significance.
          </p>
          <p>
            In this tutorial, we deem such superior capabilities brought by code modeling as <i>Code Intelligence</i>, and aim to provide a coherent overview of recent advances in this topic.
            We will start by first providing preliminaries of training foundation models on code and their common practices. We will then focus on downstream tasks in the domain of code and their evaluations. Then, we will cover how code can contribute to advancements in general tasks, and the opportunities of future research on Code Intelligence.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Schedule</h2>
        <p>
          Our tutorial will be held on Saturday, Nov 8 (all the times are based on Beijing Time = UTC+8).
          <em>Schedule may be subject to updates.</em>
        </p>

        <div class="content has-text-justified">

          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg">
          <thead>
            <tr>
              <th class="tg-0pky">Time</th>
              <th class="tg-0lax">Section</th>
              <th class="tg-0lax">Presenter</th>
              <th class="tg-0lax">Slides</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-0lax">9:00—9:15</td>
              <td class="tg-0lax">Section 1: Introduction</td>
              <td class="tg-0lax">Loubna/Terry</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-1.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">9:15—9:30</td>
              <td class="tg-0lax">Section 2: Preliminaries</td>
              <td class="tg-0lax">Terry</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-2.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">9:30—9:50</td>
              <td class="tg-0lax">Section 3: Post-training Code LMs: Supervised Fine-Tuning</td>
              <td class="tg-0lax">Wasi</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-3.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">9:50—10:15</td>
              <td class="tg-0lax">Section 4: Post-training Code LMs: Reinforcement Learning</td>
              <td class="tg-0lax">Binyuan</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-4.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">10:15—10:25</td>
              <td class="tg-0lax">Q & A Session I</td>
              <td class="tg-0lax"></td>
              <td class="tg-0lax"></td>
            </tr>
            <tr>
              <td class="tg-0lax">10:25—10:55</td>
              <td class="tg-0lax">Coffee Break</td>
              <td class="tg-0lax"></td>
              <td class="tg-0lax"></td>
            </tr>
            <tr>
              <td class="tg-0lax">10:55—11:15</td>
              <td class="tg-0lax">Section 5: Evaluating Code LMs: Function-level Code Generation</td>
              <td class="tg-0lax">Terry</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-5.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">11:15—11:35</td>
              <td class="tg-0lax">Section 6: Evaluating Code LMs: Repo-level & Agentic Code Generation</td>
              <td class="tg-0lax">Zijian</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-6.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">11:35—11:55</td>
              <td class="tg-0lax">Section 7: Bridging between Code and Natural Language</td>
              <td class="tg-0lax">Qian</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-7.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">11:55—12:10</td>
              <td class="tg-0lax">Section 8: Special Topics</td>
              <td class="tg-0lax">Terry/Loubna</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-8.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">12:10—12:20</td>
              <td class="tg-0lax">Section 9: Conclusion</td>
              <td class="tg-0lax">Terry</td>
              <td class="tg-0lax"><a href="https://code-lm.github.io/slides/sec-9.pdf">[Slides]</a></td>
            </tr>
            <tr>
              <td class="tg-0lax">12:20—12:30</td>
              <td class="tg-0lax">Q & A Session II</td>
              <td class="tg-0lax"></td>
              <td class="tg-0lax"></td>
            </tr>
          </tbody>
          </table>
        </div>
      </div>
    </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reading List</h2>

        <br />
        
        <h3 class="title is-5">Post-training Code LMs: Supervised Fine-Tuning</h3>

        <div class="content">
          <ul>
            <li>
              <b>DeepSeek-V3 Technical Report</b> (<a href="https://arxiv.org/pdf/2412.19437" target="_blank">Paper</a>)
            </li>
            <li>
              <b>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</b> (<a href="https://arxiv.org/pdf/2501.12948" target="_blank">Paper</a>)
            </li>
            <li>
              <b>Qwen3 Technical Report</b> (<a href="https://arxiv.org/pdf/2505.09388" target="_blank">Paper</a>)
            </li>
            <li>
              <b>The Llama 3 Herd of Models</b> (<a href="https://arxiv.org/pdf/2407.21783" target="_blank">Paper</a>)
            </li>
            <li>
              Magicoder: Empowering Code Generation with OSS-Instruct (<a href="https://arxiv.org/abs/2312.02120" target="_blank">Paper</a>)
            </li>
            <li>
              SelfCodeAlign: Self-Alignment for Code Generation (<a href="https://arxiv.org/abs/2406.18018" target="_blank">Paper</a>)
            </li>
            <li>
              OpenCodeInstruct: A Large-scale Instruction Tuning Dataset for Code LLMs (<a href="https://arxiv.org/abs/2402.10200" target="_blank">Paper</a>)
            </li>
            <li>
              OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models (<a href="https://arxiv.org/abs/2407.16388" target="_blank">Paper</a>)
            </li>
            <li>
              OpenCodeReasoning: Advancing Data Distillation for Competitive Coding (<a href="https://arxiv.org/abs/2501.12948" target="_blank">Paper</a>)
            </li>
            <li>
              AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy (<a href="https://arxiv.org/abs/2501.12948" target="_blank">Paper</a>)
            </li>
            <li>
              Llama 4: Multimodal Intelligence (<a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank">Blog Post</a>)
            </li>
          </ul>
        </div>
        
        <br />

        <h3 class="title is-5">Post-training Code LMs: Reinforcement Learning</h3>

        <div class="content">
          <ul>
            <li>
              <b>Execution-based Code Generation using Deep Reinforcement Learning</b> (<a href="https://arxiv.org/abs/2301.13816" target="_blank">Paper</a>)
            </li>
            <li>
              <b>RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning</b> (<a href="https://arxiv.org/abs/2410.02089" target="_blank">Paper</a>)
            </li>
            <li>
              <b>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</b> (<a href="https://arxiv.org/abs/2501.12948" target="_blank">Paper</a>)
            </li>
            <li>
              <b>DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level</b> (<a href="https://www.together.ai/blog/deepcoder" target="_blank">Blog Post</a>)
            </li>
            <li>
              <b>Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution</b> (<a href="https://arxiv.org/abs/2502.18449" target="_blank">Paper</a>)
            </li>
            <li>
              Evolving SkyRL into a Highly-Modular RL Framework (<a href="https://novasky-ai.notion.site/skyrl-v0#1ec8f0016b9d8002b700fd5431e48fc6" target="_blank">Blog Post</a>)
            </li>
            <li>
              Introducing SWE-grep and SWE-grep-mini: RL for Multi-Turn, Fast Context Retrieval (<a href="https://cognition.ai/blog/swe-grep" target="_blank">Blog Post</a>)
            </li>
            <li>
              Improving Cursor Tab with Online RL (<a href="https://cursor.com/blog/tab-rl" target="_blank">Blog Post</a>)
            </li>
          </ul>
        </div>
        
        <br />

        <h3 class="title is-5">Evaluating Code LMs: Function-level Code Generation</h3>

        <div class="content">
          <ul>
            <li>
              <b>Evaluating Large Language Models Trained on Code</b> (<a href="https://arxiv.org/abs/2107.03374" target="_blank">Paper</a>)
            </li>
            <li>
              <b>Program Synthesis with Large Language Models</b> (<a href="https://arxiv.org/abs/2108.07732" target="_blank">Paper</a>)
            </li>
            <li>
              <b>Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation</b> (<a href="https://arxiv.org/abs/2305.01210" target="_blank">Paper</a>)
            </li>
            <li>
              <b>BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions</b> (<a href="https://arxiv.org/abs/2406.15877" target="_blank">Paper</a>)
            </li>
            <li>
              BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution (<a href="https://arxiv.org/abs/2510.08697" target="_blank">Paper</a>)
            </li>
            <li>
              MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation (<a href="https://arxiv.org/abs/2208.08227" target="_blank">Paper</a>)
            </li>
            <li>
              LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code (<a href="https://arxiv.org/abs/2403.07974" target="_blank">Paper</a>)
            </li>
            <li>
              Aider Polyglot (<a href="https://aider.chat/docs/leaderboards/" target="_blank">Link</a>)
            </li>
            <li>
              Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions (<a href="https://arxiv.org/abs/2312.12450" target="_blank">Paper</a>)
            </li>
          </ul>
        </div>

        <br />

        <h3 class="title is-5">Evaluating Code LMs: Repo-level & Agentic Code Generation</h3>

        <div class="content">
          <ul>
            <li>
              <b>SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</b> (<a href="https://arxiv.org/abs/2310.06770" target="_blank">Paper</a>)
            </li>
            <li>
              <b>SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents</b> (<a href="https://arxiv.org/abs/2504.08703" target="_blank">Paper</a>)
            </li>
            <li>
              <b>Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving</b> (<a href="https://arxiv.org/abs/2504.02605" target="_blank">Paper</a>)
            </li>
            <li>
              <b>SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?</b> (<a href="https://arxiv.org/abs/2410.03859" target="_blank">Paper</a>)
            </li>
            <li>
              <b>SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?</b> (<a href="https://arxiv.org/abs/2507.12415" target="_blank">Paper</a>)
            </li>
            <li>
              SWE-Lancer: Evaluating the Economic Impact of AI on Software Engineering Freelance Markets (<a href="https://arxiv.org/abs/2502.12115" target="_blank">Paper</a>)
            </li>
            <li>
              SWE-Bench Pro: Assessing Language Models on Complex and Diverse Software Engineering Tasks (<a href="https://arxiv.org/abs/2509.16941" target="_blank">Paper</a>)
            </li>
            <li>
              SWE-Bench-Live: Real-Time Evaluation of Language Models on Live Software Engineering Issues (<a href="https://arxiv.org/abs/2505.23419" target="_blank">Paper</a>)
            </li>
            <li>
              CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion (<a href="https://arxiv.org/abs/2310.11248" target="_blank">Paper</a>)
            </li>
            <li>
              Commit0: Library Generation from Scratch (<a href="https://arxiv.org/abs/2412.01769" target="_blank">Paper</a>)
            </li>
            <li>
              Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks (<a href="https://arxiv.org/abs/2510.01359" target="_blank">Paper</a>)
            </li>
          </ul>
        </div>

        <br />

        <h3 class="title is-5">Bridging between Code and Natural Language</h3>
        
        <div class="content">
          <ul>
            <li>
              <b>Show Your Work: Scratchpads for Intermediate Computation with Language Models</b> (<a href="https://arxiv.org/abs/2112.00114" target="_blank">Paper</a>)
            </li>
            <li>
              <b>PAL: Program-aided Language Models</b> (<a href="https://arxiv.org/abs/2211.10435" target="_blank">Paper</a>)
            </li>
            <li>
              <b>ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving</b> (<a href="https://arxiv.org/abs/2309.17452" target="_blank">Paper</a>)
            </li>
            <li>
              <b>Qwen2.5-Coder Technical Report</b> (<a href="https://arxiv.org/abs/2409.12186" target="_blank">Paper</a>)
            </li>
            <li>
              <b>Lemur: Harmonizing Natural Language and Code for Language Agents</b> (<a href="https://arxiv.org/abs/2310.06830" target="_blank">Paper</a>)
            </li>
            <li>
              Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (<a href="https://arxiv.org/abs/2201.11903" target="_blank">Paper</a>)
            </li>
            <li>
              Reasoning Like Program Executors (<a href="https://arxiv.org/abs/2201.11473" target="_blank">Paper</a>)
            </li>
            <li>
              CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction (<a href="https://arxiv.org/abs/2502.07316" target="_blank">Paper</a>)
            </li>
            <li>
              CWM: An Open-Weights LLM for Research on Code Generation with World Models (<a href="https://arxiv.org/abs/2510.02387" target="_blank">Paper</a>)
            </li>
            <li>
              Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks (<a href="https://arxiv.org/abs/2211.12588" target="_blank">Paper</a>)
            </li>
            <li>
              SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning (<a href="https://arxiv.org/abs/2509.02479" target="_blank">Paper</a>)
            </li>
            <li>
              SmolLM3: smol, multilingual, long-context reasoner (<a href="https://huggingface.co/blog/smollm3" target="_blank">Blog Post</a>)
            </li>
            <li>
              Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models (<a href="https://arxiv.org/abs/2411.12580" target="_blank">Paper</a>)
            </li>
          </ul>
        </div>

      </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{zhuo2025codelm,
      title={NLP+ Code: Code Intelligence in Language Models},
      author={Zhuo, Terry Yue and Liu, Qian and Wang, Zijian and Ahmad, Wasi U and Hui, Binuian and Allal, Loubna Ben},
      booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts},
      pages={9--11},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/ACL2023-Retrieval-LM" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
